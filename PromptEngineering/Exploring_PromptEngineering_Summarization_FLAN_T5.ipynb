{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0hBXLT4PwXY"
      },
      "source": [
        "# Dialogue Summarization with FLAN-T5\n",
        "\n",
        "This notebook presents a GAI use case focused on the task of dialogue summarization. We will explore how the pre-trained FLAN-T5 language model from Hugging Face can be used to generate summaries, and how different Prompt Engineering techniques can influence the quality of the results.\n",
        "\n",
        "The objective is to demonstrate the capacity of Transformer models to understand and condense textual information, and how the proper formulation of instructions can significantly improve model performance without the need for intensive re-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cKdAe1pwPwXd"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries from Hugging Face transformers to work with T5 models.\n",
        "from transformers import AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
        "from transformers import AutoTokenizer, T5Tokenizer\n",
        "from transformers import GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZsRaJQKPwXd"
      },
      "source": [
        "## 1 - Summarizing the dialogue without prompt engineering\n",
        "\n",
        "In this section, we explore the baseline performance of the pre-trained FLAN-T5 LLM from Hugging Face in generating summaries of dialogues without the use of specific prompt engineering techniques. The goal is to see how the model performs when simply presented with the dialogue text.\n",
        "\n",
        "The list of available models in the Hugging Face `transformers` library can be found [here](https://huggingface.co/docs/transformers/index)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are some example dialogues along with their corresponding human-created summaries. These examples will be used to evaluate the model's performance in the following sections."
      ],
      "metadata": {
        "id": "JsJD1kfFj0jH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6H18iJN0PwXe"
      },
      "outputs": [],
      "source": [
        "ejemplo_dialogo_1 = \"\"\"\n",
        "#Person1#: What time is it, Tom?\n",
        "#Person2#: Just a minute. It's ten to nine by my watch.\n",
        "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
        "#Person2#: What's the hurry?\n",
        "#Person1#: I must catch the nine-thirty train.\n",
        "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
        "\"\"\"\n",
        "ejemplo_resumen_1 = \"\"\"\n",
        "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
        "\"\"\"\n",
        "ejemplo_dialogo_2 = \"\"\"\n",
        "#Person1#: May, do you mind helping me prepare for the picnic?\n",
        "#Person2#: Sure. Have you checked the weather report?\n",
        "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
        "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
        "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
        "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
        "#Person1#: All set. May, can you help me take all these things to the living room?\n",
        "#Person2#: Yes, madam.\n",
        "#Person1#: Ask Daniel to give you a hand?\n",
        "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
        "\"\"\"\n",
        "ejemplo_resumen_2 = \"\"\"\n",
        "Mom asks May to help to prepare for the picnic and May agrees.\n",
        "\"\"\"\n",
        "ejemplo_dialogo_3 = \"\"\"\n",
        "#Person1#: Have you considered upgrading your system?\n",
        "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
        "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
        "#Person2#: That would be a definite bonus.\n",
        "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
        "#Person2#: How can we do that?\n",
        "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
        "#Person2#: No.\n",
        "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
        "#Person2#: That sounds great. Thanks.\n",
        "\"\"\"\n",
        "ejemplo_resumen_3 = \"\"\"\n",
        "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
        "\"\"\"\n",
        "ejemplo_dialogo_4 = \"\"\"\n",
        "#Person1#: Hello, I bought the pendant in your shop, just before.\n",
        "#Person2#: Yes. Thank you very much.\n",
        "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid.\n",
        "#Person2#: Oh, is it?\n",
        "#Person1#: Would you change it to a new one?\n",
        "#Person2#: Yes, certainly. You have the receipt?\n",
        "#Person1#: Yes, I do.\n",
        "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it.\n",
        "#Person1#: Thank you so much.\n",
        "\"\"\"\n",
        "ejemplo_resumen_4 = \"\"\"\n",
        "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewMMaKI4PwXf",
        "outputId": "675f9ac7-cbf8-41a2-ff49-71c564446b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Person1#: What time is it, Tom? #Person2#: Just a minute. It's ten to nine by my watch. #Person1#: Is it? I had no idea it was so late. I must be off now. #Person2#: What's the hurry? #Person1#: I must catch the nine-thirty train. #Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n"
          ]
        }
      ],
      "source": [
        "print(ejemplo_dialogo_1.strip().replace(\"\\n\",\" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, the example dialogues and their human-created summaries are printed, separated by dashed lines for clarity."
      ],
      "metadata": {
        "id": "yfiMfJ0Lke15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn532VEsPwXg",
        "outputId": "5f46d346-ddb0-496a-f672-d8caa97945e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Ejemplo  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGO DE ENTRADA:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUMEN HECHO POR UNA HUMANO:\n",
            "\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Ejemplo  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGO DE ENTRADA:\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUMEN HECHO POR UNA HUMANO:\n",
            "\n",
            "Mom asks May to help to prepare for the picnic and May agrees.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Ejemplo  3\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGO DE ENTRADA:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUMEN HECHO POR UNA HUMANO:\n",
            "\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Ejemplo  4\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGO DE ENTRADA:\n",
            "\n",
            "#Person1#: Hello, I bought the pendant in your shop, just before.\n",
            "#Person2#: Yes. Thank you very much.\n",
            "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid.\n",
            "#Person2#: Oh, is it?\n",
            "#Person1#: Would you change it to a new one?\n",
            "#Person2#: Yes, certainly. You have the receipt?\n",
            "#Person1#: Yes, I do.\n",
            "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it.\n",
            "#Person1#: Thank you so much.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUMEN HECHO POR UNA HUMANO:\n",
            "\n",
            "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "linea_punteada = '-'.join('' for x in range(100))\n",
        "\n",
        "dialogos = [ejemplo_dialogo_1, ejemplo_dialogo_2, ejemplo_dialogo_3, ejemplo_dialogo_4]\n",
        "resumenes = [ejemplo_resumen_1, ejemplo_resumen_2, ejemplo_resumen_3, ejemplo_resumen_4]\n",
        "\n",
        "for i, (dialogo, resumen) in enumerate(zip(dialogos, resumenes)):\n",
        "    print(linea_punteada)\n",
        "    print('Ejemplo ', i + 1)\n",
        "    print(linea_punteada)\n",
        "    print('DIALOGO DE ENTRADA:')\n",
        "    print(dialogo)\n",
        "    print(linea_punteada)\n",
        "    print('RESUMEN HECHO POR UNA HUMANO:')\n",
        "    print(resumen)\n",
        "    print(linea_punteada)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3p0jGYPwXh"
      },
      "source": [
        "Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) by creating an instance of the `AutoModelForSeq2SeqLM` class using the `.from_pretrained()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20WsTqgsPwXh",
        "outputId": "e81be5ff-b853-486f-df51-401eba52fc22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "nombre_modelo='google/flan-t5-base'\n",
        "modelo = AutoModelForSeq2SeqLM.from_pretrained(nombre_modelo)\n",
        "modelo.to('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KLUY7ogPwXi"
      },
      "source": [
        "Download the tokenizer for the FLAN-T5 model by using the `AutoTokenizer.from_pretrained() ` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RkzTyxxSPwXi"
      },
      "outputs": [],
      "source": [
        "tokenizador = AutoTokenizer.from_pretrained(nombre_modelo, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNyjSjdGPwXj"
      },
      "source": [
        "Let's try with a different codification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llj48hFZPwXj",
        "outputId": "de8c1c1e-612f-45fc-e2a2-8751a9fe369a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embbeddings:\n",
            "tensor([ 2645,    33,    25, 11454,     9,    89,  4636,    49,    58,     1])\n",
            "\n",
            "Original Sentence:\n",
            "Who are you madafucker?\n"
          ]
        }
      ],
      "source": [
        "oracion = \"What time is it, Tom?\"\n",
        "oracion = \"Who are you madafucker?\"\n",
        "\n",
        "oracion_codificada = tokenizador(oracion, return_tensors='pt')\n",
        "\n",
        "oracion_decodificada = tokenizador.decode(\n",
        "        oracion_codificada[\"input_ids\"][0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print('Embbeddings:')\n",
        "print(oracion_codificada[\"input_ids\"][0])\n",
        "print('\\nOriginal Sentence:')\n",
        "print(oracion_decodificada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubUA1a5TPwXj"
      },
      "source": [
        "Now it's time to explore how well the LLM summarizes a dialogue without prompt engineering. **Prompt engineering** is the act of a human changing the **prompt/instruction/input** to improve the response to a given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUcjFPkUPwXk",
        "outputId": "b34eb83e-8bc5-4af6-cbb1-61eaacad3dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Prompt input:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Models output without prompt engineering:\n",
            "Person1: It's ten to nine.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Prompt input:\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "Mom asks May to help to prepare for the picnic and May agrees.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Models output without prompt engineering:\n",
            "#Person1#: May, can you help me prepare the picnic?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (dialogo, resumen) in enumerate(zip(dialogos[:-2], resumenes[:-2])):\n",
        "    entradas = tokenizador(dialogo, return_tensors='pt')\n",
        "    salida = tokenizador.decode(\n",
        "        modelo.generate(\n",
        "            entradas[\"input_ids\"].to('cuda'),\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(linea_punteada)\n",
        "    print('Example ', i + 1)\n",
        "    print(linea_punteada)\n",
        "    print(f'Prompt input:\\n{dialogo}')\n",
        "    print(linea_punteada)\n",
        "    print(f'Summary by human:\\n{resumen}')\n",
        "    print(linea_punteada)\n",
        "    print(f'Models output without prompt engineering:\\n{salida}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96K5iEndPwXl"
      },
      "source": [
        "It can be seen that the model's assumptions make some sense, but it doesn't seem sure what task it is supposed to perform. It seems to just invent the next sentence in the dialogue. Prompt engineering can help in this case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb1mfe6pPwXl"
      },
      "source": [
        "## 3 - Summarizing the dialogue by being explicit with the input instruction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXKW-oc0PwXl"
      },
      "source": [
        "###  3.1 - Zero-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuUkokQ4PwXm",
        "outputId": "fdf14cae-1b81-4faa-d899-f1e65a18870e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Exmple  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Prompt input:\n",
            "\n",
            "              Summarize the following conversation.\n",
            "\n",
            "              \n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "\n",
            "              Summary:\n",
            "              \n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Models output - zero shot:\n",
            "The train is about to leave.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Exmple  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Prompt input:\n",
            "\n",
            "              Summarize the following conversation.\n",
            "\n",
            "              \n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "\n",
            "              Summary:\n",
            "              \n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "Mom asks May to help to prepare for the picnic and May agrees.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Models output - zero shot:\n",
            "The weather report says it will be sunny all day.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (dialogo, resumen) in enumerate(zip(dialogos[:-2], resumenes[:-2])):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "              Summarize the following conversation.\n",
        "\n",
        "              {dialogo}\n",
        "\n",
        "              Summary:\n",
        "              \"\"\"\n",
        "\n",
        "    entradas = tokenizador(prompt, return_tensors='pt')\n",
        "    salida = tokenizador.decode(\n",
        "        modelo.generate(\n",
        "            entradas[\"input_ids\"].to('cuda'),\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(linea_punteada)\n",
        "    print('Exmple ', i + 1)\n",
        "    print(linea_punteada)\n",
        "    print(f'Prompt input:\\n{prompt}')\n",
        "    print(linea_punteada)\n",
        "    print(f'Summary by human:\\n{resumen}')\n",
        "    print(linea_punteada)\n",
        "    print(f'Models output - zero shot:\\n{salida}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7YhtBVsPwXm"
      },
      "source": [
        "This is much better! But the model still doesn't capture the nuances of the conversations..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU-k7w_XPwXm"
      },
      "source": [
        "###  3.2 - Templates for FLAN-T5\n",
        "\n",
        "It's worth mentioning that FLAN-T5 has many message templates for specific tasks. You can find the official FLAN-T5 predefined message templates [here](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py).\n",
        "\n",
        "This point is informative for further experimentation. Since these \"official\" templates are in English, you could either run the model in English or try running these templates with their respective translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "gcKSj91JUOK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3030d50c-eed9-4598-da9e-7bd67595c38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Prompt input:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "\n",
            "What was going on?\n",
            "    \n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Output - ZERO SHOT:\n",
            "Tom is late for the train.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Prompt input:\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "\n",
            "What was going on?\n",
            "    \n",
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "Mom asks May to help to prepare for the picnic and May agrees.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Output - ZERO SHOT:\n",
            "Person1 wants to prepare for the picnic. May will help her.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (dialogo, resumen) in enumerate(zip(dialogos[:-2], resumenes[:-2])):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogo}\n",
        "\n",
        "What was going on?\n",
        "    \"\"\"\n",
        "\n",
        "    entradas = tokenizador(prompt, return_tensors='pt')\n",
        "    salida = tokenizador.decode(\n",
        "        modelo.generate(\n",
        "            entradas[\"input_ids\"].to('cuda'),\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(linea_punteada)\n",
        "    print('Example ', i + 1)\n",
        "    print(linea_punteada)\n",
        "    print(f'Prompt input:\\n{prompt}')\n",
        "    print(linea_punteada)\n",
        "    print(f'Summary by human:\\n{resumen}')\n",
        "    print(linea_punteada)\n",
        "    print(f'Output - ZERO SHOT:\\n{salida}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsOK-U3kPwXm"
      },
      "source": [
        "##  4 - One and Few-Shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVTfjzOvPwXm"
      },
      "source": [
        "###  4.1 - One-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vldcxHLAPwXn"
      },
      "outputs": [],
      "source": [
        "def crear_prompt(indices_ejemplo_prompt, indice_a_resumir):\n",
        "    prompt = ''\n",
        "    for indice in indices_ejemplo_prompt:\n",
        "        dialogo = dialogos[indice]\n",
        "        resumen = resumenes[indice]\n",
        "\n",
        "        prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogo}\n",
        "\n",
        "What was going on?:\n",
        "{resumen}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    ejemplo_a_resumir = dialogos[indice_a_resumir]\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{ejemplo_a_resumir}\n",
        "\n",
        "What was going on?\n",
        "\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94HxY7zPPwXn"
      },
      "source": [
        "\n",
        "Construya el mensaje para realizar una inferencia One-Shot:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZfoDJ5WPwXn",
        "outputId": "7c50fff0-515f-45ec-8e0f-ef57c794b171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "\n",
            "What was going on?:\n",
            "\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "\n",
            "What was going on?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indices_ejemplo_prompt = [0]\n",
        "indice_a_resumir = 1\n",
        "\n",
        "one_shot_prompt = crear_prompt(indices_ejemplo_prompt, indice_a_resumir)\n",
        "\n",
        "print(one_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5hM8gcwPwXn"
      },
      "source": [
        "Let's try one shot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9poqrWSBPwXo",
        "outputId": "88bc0b21-388e-4220-a9f2-94b0a89492bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Output - ONE SHOT:\n",
            "#Person1 wants to prepare for the picnic. May will help her.\n"
          ]
        }
      ],
      "source": [
        "resumen = dialogos[indice_a_resumir]\n",
        "\n",
        "entradas = tokenizador(one_shot_prompt, return_tensors='pt')\n",
        "salida = tokenizador.decode(\n",
        "    modelo.generate(\n",
        "        entradas[\"input_ids\"].to('cuda'),\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(linea_punteada)\n",
        "print(f'Summary by human:\\n{resumen}\\n')\n",
        "print(linea_punteada)\n",
        "print(f'Output - ONE SHOT:\\n{salida}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysru_GirPwXp"
      },
      "source": [
        "###  4.2 - Few-Shot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdEexnSQPwXp",
        "outputId": "094796cd-8ef5-41f3-a62e-b8cf2406bd58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "\n",
            "What was going on?:\n",
            "\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "\n",
            "What was going on?:\n",
            "\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: Hello, I bought the pendant in your shop, just before.\n",
            "#Person2#: Yes. Thank you very much.\n",
            "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid.\n",
            "#Person2#: Oh, is it?\n",
            "#Person1#: Would you change it to a new one?\n",
            "#Person2#: Yes, certainly. You have the receipt?\n",
            "#Person1#: Yes, I do.\n",
            "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it.\n",
            "#Person1#: Thank you so much.\n",
            "\n",
            "\n",
            "What was going on?:\n",
            "\n",
            "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "\n",
            "What was going on?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indices_ejemplo_prompt = [0, 2, 3]\n",
        "indice_a_resumir = 1\n",
        "\n",
        "few_shot_prompt = crear_prompt(indices_ejemplo_prompt, indice_a_resumir)\n",
        "\n",
        "print(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9x5jcJhPwXp"
      },
      "source": [
        "Now pass this prompt to perform a few shot inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUZrx5CgPwXq",
        "outputId": "fa0a909b-b801-4631-dd93-41129cbea509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Summary by human:\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Output:\n",
            "#Person1 wants to prepare for the picnic. She asks her mother to help her prepare.\n"
          ]
        }
      ],
      "source": [
        "resumen = dialogos[indice_a_resumir]\n",
        "\n",
        "entradas = tokenizador(few_shot_prompt, return_tensors='pt')\n",
        "salida = tokenizador.decode(\n",
        "    modelo.generate(\n",
        "        entradas[\"input_ids\"].to('cuda'),\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(linea_punteada)\n",
        "print(f'Summary by human:\\n{resumen}\\n')\n",
        "print(linea_punteada)\n",
        "print(f'Output:\\n{salida}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7s9fz0bPwXq"
      },
      "source": [
        "In this case, Few-Shot inference did not provide a significant improvement over One-Shot. And any value greater than 5 or 6 instructions within Few-Shot is usually not very helpful either. In addition, you must make sure not to exceed the model's input context length, which in our case is 512 tokens. Any value exceeding the context length will be ignored.\n",
        "\n",
        "However, it can be observed that introducing at least one complete example (One-shot) is sufficient, providing the model with more information and qualitatively improving the overall summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv3CMbOEPwXr"
      },
      "source": [
        "##  5 - Parameters for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S0UvQcgPwXr"
      },
      "source": [
        "Let's change the configuration parameters of the `generate()` method to see different output from the LLM. So far, the only parameter that has been set was `max_new_tokens=50`, which defines the maximum number of tokens to generate. You can find a complete list of available parameters in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig).\n",
        "\n",
        "A convenient way to organize the configuration parameters is to use the `GenerationConfig` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeUDJbUvPwXr"
      },
      "source": [
        "By setting the parameter `do_sample = True`, several decoding strategies are activated that influence the next token from the probability distribution across the entire vocabulary. Then, you can fine-tune the outputs by modifying `temperature` and other parameters (such as `top_k` and `top_p`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41dbDbbjPwXr",
        "outputId": "b21ea475-fa99-46cc-e381-1076eee86325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "RESUMEN HECHO POR UN HUMANO:\n",
            "\n",
            "#Person1#: May, do you mind helping me prepare for the picnic?\n",
            "#Person2#: Sure. Have you checked the weather report?\n",
            "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
            "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
            "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
            "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
            "#Person1#: All set. May, can you help me take all these things to the living room?\n",
            "#Person2#: Yes, madam.\n",
            "#Person1#: Ask Daniel to give you a hand?\n",
            "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "GENERACION/SALIDA DEL MODELO - ONE SHOT:\n",
            "#Person1 wants to make a sandwich and toast for Daniel and his family. She asks her mother to help her prepare the food.\n"
          ]
        }
      ],
      "source": [
        "# generacion_config = GenerationConfig(max_new_tokens=50)\n",
        "# generacion_config = GenerationConfig(max_new_tokens=10)\n",
        "# generacion_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
        "generacion_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
        "# generacion_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
        "\n",
        "entradas = tokenizador(few_shot_prompt, return_tensors='pt')\n",
        "salida = tokenizador.decode(\n",
        "    modelo.generate(\n",
        "        entradas[\"input_ids\"].to('cuda'),\n",
        "        generation_config=generacion_config,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(linea_punteada)\n",
        "print(f'RESUMEN HECHO POR UN HUMANO:\\n{resumen}\\n')\n",
        "print(linea_punteada)\n",
        "print(f'GENERACION/SALIDA DEL MODELO - ONE SHOT:\\n{salida}')"
      ]
    },
    {
      "source": [
        "git push -u origin main"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LxI8qVbPqHaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}