{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRIhMcvvyfbu"
   },
   "source": [
    "# Few-Shot Learning, Chain of Thought, and Instructions for Mistral 7B\n",
    "\n",
    "This notebook explores various prompting techniques for interacting with the Mistral 7B Instruct model. You will learn how to leverage Few-Shot Learning, Chain of Thought, and specific instructions to guide the model's responses effectively. By the end of this notebook, you should have a better understanding of how to craft prompts that yield more accurate and desirable outputs from the Mistral 7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7cL0wfAS3gbo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('YOUR_VARIABLE_NAME')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2vH0_NYEzD_O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXJEajYe4Xcs"
   },
   "source": [
    "## A brief information about Mistral 7B Instruct v0.2.\n",
    "\n",
    "So, I've been checking out this model called Mistral 7B Instruct v0.2. It's a really capable language model from Mistral AI, and the cool part is that it's the 'Instruct' version. That means they specifically trained it to follow instructions, which is super handy for things like generating text, answering questions, or even translating stuff. It's only 7 billion parameters, so it's pretty efficient compared to some of the massive models out there, making it a good choice if you don't have a ton of computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "afdd597fe5f1453d97d4ea984797e503",
      "3c096239770f43d1bdedd79c98910830",
      "186e01a49698488b8f942b707320c1ba",
      "d519766cf3eb4088abf992182f7baa24",
      "01d9027b513c48938e9955348835765f",
      "0f50aad6bd654b128ea1491365be75c5",
      "54afb4e07a5b435da9727a89970e315f",
      "27ec58b940914160b6d1c129c73f2685",
      "026681dcf45b4084bac66c18dd2b2a9e",
      "db27d81d4f3f4ebe8735641ec5c33cf7",
      "e081a0f0e5f94ce6ab4c37713e87c666"
     ]
    },
    "id": "EqNV3i4CzGHp",
    "outputId": "5b7b6bd7-c1e5-4d83-f94e-76d394a51581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdd597fe5f1453d97d4ea984797e503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\" if USE_GPU else \"cpu\",\n",
    "    torch_dtype=torch.float16 if USE_GPU else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_folder=\"offload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zdufk-d35sIT",
    "outputId": "46db049a-9f36-4081-c280-1589ec507ee6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Generated Response:\n",
      "How does artificial intelligence work?\n",
      "\n",
      "Artificial intelligence, or AI, refers to the ability of a computer program or machine to mimic intelligent human behavior, such as learning, problem solving, and decision making. AI systems are designed to analyze data, identify patterns, and make decisions based on that data.\n",
      "\n",
      "There are several different approaches to building AI systems, including:\n",
      "\n",
      "1. Rule-based systems: These systems use a set of predefined rules to make decisions. For example, a rule-based system might be used to identify fraud in financial transactions based on a set of rules defined by experts.\n",
      "2. Machine learning: Machine learning algorithms use data to learn patterns and make decisions. These algorithms can be divided into two main categories: supervised learning and unsupervised learning. Supervised learning algorithms are trained on labeled data, meaning that the data comes with known outcomes. Unsupervised learning algorithms are used to find patterns in unlabeled data, meaning that the data does not come with known outcomes.\n",
      "3. Deep learning: Deep learning is a type of machine learning that uses artificial neural networks to model and solve complex problems. Neural networks are modeled after the structure and function of the human brain, with layers of interconnected\n"
     ]
    }
   ],
   "source": [
    "# Function to generate text with optimized Mistral-7B\n",
    "def generate_response(prompt, max_length=256):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Optimized text generation\n",
    "    with torch.no_grad():  # Reduce memory usage\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,  # Allow variability\n",
    "            temperature=0.7,  # Control creativity\n",
    "            top_p=0.9  # Avoid random responses\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "input_text = \"How does artificial intelligence work?\"\n",
    "response = generate_response(input_text)\n",
    "\n",
    "print(\"\\n Generated Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYMdtKZm8zRS"
   },
   "source": [
    "Genral function to call the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UOnxUyqF8NfM"
   },
   "outputs": [],
   "source": [
    "# Generation function with adjustable parameters\n",
    "def generate_response(prompt, max_length=512, temperature=0.2, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikMwnhGo8VMD"
   },
   "source": [
    "## Few-shots learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Va05joQr8yPF"
   },
   "outputs": [],
   "source": [
    "# Implementación de Few-Shot Learning\n",
    "def few_shot_learning(prompt):\n",
    "    print(\"\\n Few-Shot Learning:\")\n",
    "    answer = generate_response(prompt)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMFYu81o8a6h"
   },
   "source": [
    "### Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxO5XOPD8YS8",
    "outputId": "3c64fb3c-f5cb-4523-fb2d-029fb78d61b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Few-Shot Learning:\n",
      "\n",
      "          Translate the following sentences to french:\n",
      "          'Hola' → 'Bonjour'\n",
      "          'Gracias' → 'Merci'\n",
      "          'Adiós' →\n",
      "          'Bonito día' →\n",
      "          'Hasta luego' →\n",
      "          'Con cuidado' →\n",
      "          -----------------------------------------------------------------------\n",
      "          'Adiós' → 'Au revoir'\n",
      "          'Bonito día' → 'Jolie journée'\n",
      "          'Hasta luego' → 'À bientôt'\n",
      "          'Con cuidado' → 'Avec soin'\n",
      "          'Por favor' → 'S'il vous plaît'\n",
      "          'Gracias mil' → 'Merci beaucoup'\n",
      "          'De nada' → 'De rien'\n",
      "          'Lo siento' → 'Desolé'\n",
      "          'Hola' → 'Bonjour'\n",
      "          'Puedes' → 'Peux-tu'\n",
      "          'Muchas gracias' → 'Merci beaucoup beaucoup'\n",
      "          'No' → 'Non'\n",
      "          'Sí' → '\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "          Translate the following sentences to french:\n",
    "          'Hola' → 'Bonjour'\n",
    "          'Gracias' → 'Merci'\n",
    "          'Adiós' →\n",
    "          'Bonito día' →\n",
    "          'Hasta luego' →\n",
    "          'Con cuidado' →\n",
    "          \"\"\"\n",
    "\n",
    "few_shot_learning(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luw-fR4g-g2T",
    "outputId": "82c7e9a6-28e6-4225-fe25-26791001f577"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With few-shot prompt\n",
      "\n",
      " Few-Shot Learning:\n",
      "\n",
      "                  You are an intelligent agent that translates phrases from Spanish to French.\n",
      "                  Translate the following in the following format: 'Spanish phrase' → 'French phrase'\n",
      "\n",
      "                  Here are some examples:\n",
      "                  'Hola' → 'Bonjour'\n",
      "                  'Gracias' → 'Merci'\n",
      "\n",
      "                  Now translate these phrases. ONLY provide the translations for the following phrases:\n",
      "                  'Adiós' →\n",
      "                  'Bonito día' →\n",
      "                  'Hasta luego' →\n",
      "                  'Con cuidado' →\n",
      "                  \n",
      "                  'Adiós' → 'Au revoir'\n",
      "                  'Bonito día' → 'Joli jour' (or 'Beau jour' for masculine contexts)\n",
      "                  'Hasta luego' → 'À bientôt'\n",
      "                  'Con cuidado' → 'Avec soin' or 'Prenez soin de vous' (for 'take care of yourself')\n"
     ]
    }
   ],
   "source": [
    "print(\"With few-shot prompt\")\n",
    "prompt_few_shot = \"\"\"\n",
    "                  You are an intelligent agent that translates phrases from Spanish to French.\n",
    "                  Translate the following in the following format: 'Spanish phrase' → 'French phrase'\n",
    "\n",
    "                  Here are some examples:\n",
    "                  'Hola' → 'Bonjour'\n",
    "                  'Gracias' → 'Merci'\n",
    "\n",
    "                  Now translate these phrases. ONLY provide the translations for the following phrases:\n",
    "                  'Adiós' →\n",
    "                  'Bonito día' →\n",
    "                  'Hasta luego' →\n",
    "                  'Con cuidado' →\n",
    "                  \"\"\"\n",
    "few_shot_learning(prompt_few_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHgtuV6rAlNR"
   },
   "source": [
    "Comparing these two, it's clear: the more precise and explicit you are with your instructions to a large language model, the better and more controlled its output will be.\n",
    "\n",
    "The AI in both cases understood the core task (Spanish to French translation) from just a couple of examples – that's the \"few-shot\" magic. But by clearly defining its role, setting strict boundaries on the output, and perhaps even hinting at the level of detail we expect, we moved from a good, but overly eager, response to one that was perfectly aligned with our needs and even more insightful. It's like guiding a very smart, but sometimes overly enthusiastic, assistant. A little extra clarity goes a very long way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbjLs88GBfkW"
   },
   "source": [
    "## Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "prnr1g5_855z"
   },
   "outputs": [],
   "source": [
    "# Implementación de Chain of Thought (CoT)\n",
    "def chain_of_thought(prompt):\n",
    "    print(\"\\n Chain of Thought:\")\n",
    "    answer = generate_response(prompt)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VE8pm5RkBrVg",
    "outputId": "923424d3-a376-44ee-9949-12b13fe6bf91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Chain of Thought:\n",
      "\n",
      "  Solve 27 × 19 by explaing each step you take.\n",
      "\n",
      "  To solve 27 × 19, we can use the multiplication algorithm that we learned in elementary school. This method is also known as the \"long multiplication\" method.\n",
      "\n",
      "  Step 1: Set up the problem. Write down the numbers in the correct order:\n",
      "\n",
      "          ________\n",
      "         |     |   |\n",
      "27    |     |   |19\n",
      "         |_____|___|\n",
      "\n",
      "  Step 2: Multiply each digit in the first number (27) by each digit in the second number (19), and write the results below. If any product has more than one digit, carry the extra digit to the next column.\n",
      "\n",
      "          ________\n",
      "         | 502  |   |\n",
      "27    |  _____|___|19\n",
      "         |_____|___|\n",
      "\n",
      "  Step 3: Write down the number of zeros that correspond to the number of digits in the second number (1 for 19, so we write down 1 zero).\n",
      "\n",
      "          ________\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "  Solve 27 × 19 by explainig each step you take.\n",
    "\"\"\"\n",
    "\n",
    "chain_of_thought(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_b4ZrygB0SR",
    "outputId": "39ed0962-245a-4e28-88e4-58ae29583025"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Chain of Thought:\n",
      "\n",
      "You are an intelligent agent that solves multiplication problems by explaining each step in detail and horizontally.\n",
      "\n",
      "Example of how you should do it:\n",
      "\n",
      "Problem: 32 x 80\n",
      "Solution:\n",
      "1. The operation is: 32 x 80\n",
      "2. We separate the first number into tens and units: (30 + 2) x 80\n",
      "3. We distribute: 30 x 80 + 2 x 80\n",
      "4. We start simplifying: 2400 + 160\n",
      "5. We get the final result: 2560\n",
      "\n",
      "Now solve the following problem. DO NOT add anything else:\n",
      "\n",
      "Problem: 27 × 19\n",
      "Solution:\n",
      "1. The operation is: 27 × 19\n",
      "2. We separate the first number into tens and units: (20 + 7) × 19\n",
      "3. We distribute: 20 × 19 + 7 × 19\n",
      "4. We start simplifying: 380 + 133\n",
      "5. We get the final result: \n"
     ]
    }
   ],
   "source": [
    "prompt_chain_of_thought = \"\"\"\n",
    "You are an intelligent agent that solves multiplication problems by explaining each step in detail and horizontally.\n",
    "\n",
    "Example of how you should do it:\n",
    "\n",
    "Problem: 32 x 80\n",
    "Solution:\n",
    "1. The operation is: 32 x 80\n",
    "2. We separate the first number into tens and units: (30 + 2) x 80\n",
    "3. We distribute: 30 x 80 + 2 x 80\n",
    "4. We start simplifying: 2400 + 160\n",
    "5. We get the final result: 2560\n",
    "\n",
    "Now solve the following problem. DO NOT add anything else:\n",
    "\n",
    "Problem: 27 × 19\n",
    "Solution:\n",
    "\"\"\"\n",
    "\n",
    "chain_of_thought(prompt_chain_of_thought)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UMyCxYwC7cX"
   },
   "source": [
    "The comparison is striking: when we provide the AI with a clear example of how we want it to think and break down a problem (a Chain of Thought), it becomes much more accurate and controlled in its output.\n",
    "\n",
    "The first example shows the model trying its best, but without a clear \"thinking\" template, it might generate a less optimal or confusing solution. The second example demonstrates that by giving it a structured thought process (our example multiplication), the AI can adopt that specific reasoning pathway, leading to a much more precise and predictable result. It's like giving a student a detailed worked example; they're far more likely to follow that exact method for future problems. For complex tasks, guiding the AI's internal steps is truly transformative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMwGm7TKDDll"
   },
   "source": [
    "# Specific instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Udi82H0p9Az1"
   },
   "outputs": [],
   "source": [
    "# Implementation of Specific Instructions\n",
    "def specific_instructions(prompt):\n",
    "    print(\"\\n Specific Instructions:\")\n",
    "    response = generate_response(prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jeS0hkBrIhr",
    "outputId": "8fe05034-1565-4dc2-d392-62be4736724d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Specific Instructions:\n",
      "\n",
      "Respond in JSON format with the following keys: 'name', 'age', and 'profession'.\n",
      "Input example: \"Juan Pérez, 35 years old, engineer\"\n",
      "Output example: {\n",
      "  \"name\": \"Juan Pérez\",\n",
      "  \"age\": 35,\n",
      "  \"profession\": \"engineer\"\n",
      "}\n",
      "input = \"Mike Johnson, 27 years old, doctor\"\n",
      "output = {\n",
      "  \"name\": \"Mike Johnson\",\n",
      "  \"age\": 27,\n",
      "  \"profession\": \"doctor\"\n",
      "}\n",
      "input = \"Sara Brown, 42 years old, teacher\"\n",
      "output = {\n",
      "  \"name\": \"Sara Brown\",\n",
      "  \"age\": 42,\n",
      "  \"profession\": \"teacher\"\n",
      "}\n",
      "input = \"Mark Davis, 31 years old, artist\"\n",
      "output = {\n",
      "  \"name\": \"Mark Davis\",\n",
      "  \"age\": 31,\n",
      "  \"profession\": \"artist\"\n",
      "}\n",
      "input = \"Laura Smith, 29 years old, lawyer\"\n",
      "output = {\n",
      "  \"name\": \"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Respond in JSON format with the following keys: 'name', 'age', and 'profession'.\n",
    "Input example: \"Juan Pérez, 35 years old, engineer\"\n",
    "\"\"\"\n",
    "\n",
    "specific_instructions(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EGYH_meD48u",
    "outputId": "39ad9f89-ff47-4b4b-d171-8136dffff9af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Specific Instructions:\n",
      "\n",
      "Respond in JSON format with the following keys: 'name', 'age', and 'profession'.\n",
      "DON'T add any other text or explanation.\n",
      "\n",
      "Input example: \"Juan Pérez, 35 years old, engineer\"\n",
      "\n",
      "Output example: {\"name\": \"Juan Pérez\", \"age\": 35, \"profession\": \"engineer\"}\n",
      "\n",
      "name: \"John Doe\",\n",
      "age: 42,\n",
      "profession: \"doctor\"\n"
     ]
    }
   ],
   "source": [
    "prompt_specific = \"\"\"\n",
    "Respond in JSON format with the following keys: 'name', 'age', and 'profession'.\n",
    "DON'T add any other text or explanation.\n",
    "\n",
    "Input example: \"Juan Pérez, 35 years old, engineer\"\n",
    "\"\"\"\n",
    "\n",
    "specific_instructions(prompt_specific)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
