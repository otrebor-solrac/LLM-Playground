{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00377e4-5d0c-4f46-b2ff-8487506d31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re,os\n",
    "\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from tqdm import tqdm\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a4223f-f528-4a98-bd22-caa7ccdd4a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed82014-e816-462f-b016-98be01b67753",
   "metadata": {},
   "source": [
    "# Simple RAG script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c07cc3-0bb9-4a17-8a05-d1bfecc725f9",
   "metadata": {},
   "source": [
    "Welcome to this notebook, where we've been building a foundational Retrieval-Augmented Generation (RAG) system. Our journey began by extracting text from PDF documents using OCR, turning unstructured data into usable text. Recognizing that Large Language Models (LLMs) have limited context windows, we then employed a recursive text splitter to meticulously break down these large texts into smaller, manageable chunks, ensuring a crucial overlap between them to preserve semantic continuity.\n",
    "\n",
    "Following this, we transformed these text chunks into numerical representations called embeddings using OpenAI's embedding models. These embeddings are vital as they allow computers to understand and compare text based on its meaning, rather than just keywords. Finally, we've taken these embeddings and indexed them within a vector database, specifically utilizing FAISS or ChromaDB. This vector store is the backbone of our RAG system, enabling fast and efficient semantic searches to retrieve the most relevant information. This entire pipeline allows us to effectively leverage external knowledge bases to provide LLMs with precise, context-rich information, ultimately leading to more accurate and informed generations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23ef47-f233-4b52-8b04-c50293e14ec5",
   "metadata": {},
   "source": [
    "# Select model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f00b0-9942-4933-90f8-6b145c44f9e5",
   "metadata": {},
   "source": [
    "Prepare the OpenAI gpt-3.5-turbo-16k model for use in subsequent tasks, likely for generating conversational responses, summaries, or any other language processing task that requires a chat model with a large context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2ea628c-5711-4812-86bf-99db6df2ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-4.1-nano'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9889944-20de-49be-be19-fd2f46d24021",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb24319-d622-4e87-b37a-4183ce5406a1",
   "metadata": {},
   "source": [
    "## Load webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d370a99-312e-4297-bbad-b4565ade3e49",
   "metadata": {},
   "source": [
    "Let's try with a web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea854bb6-7d66-4804-bacd-d0d25ef1e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/History_of_artificial_intelligence\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55655a99-e2f4-46cb-aab2-da1210464727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "'\\n\\n\\n\\nHistory of artificial intelligence - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate C'\n",
      "\n",
      "Clean text:\n",
      "'History of artificial intelligence - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us Contribute HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages Search Search Appearance Donate Create account Log in Personal tools Donate C'\n"
     ]
    }
   ],
   "source": [
    "text = docs[0].page_content[:500]\n",
    "\n",
    "new_text = text.replace('\\n', ' ')\n",
    "new_text = re.sub(r'\\s+', ' ', new_text)\n",
    "\n",
    "final_text = new_text.strip()\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(repr(text))\n",
    "print(\"\\nClean text:\")\n",
    "print(repr(final_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc0e34-630b-4df9-98d6-9239c667aa27",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e4c9c-6c00-47ff-8064-99fef244bfd7",
   "metadata": {},
   "source": [
    "### By using OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33536a7e-a9bd-4563-898c-7856946ca1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':   7%|▋         | 1/14 [00:05<01:10,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 1 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  14%|█▍        | 2/14 [00:09<00:53,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 2 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page001.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  21%|██▏       | 3/14 [00:14<00:52,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 3 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page002.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  29%|██▊       | 4/14 [00:18<00:46,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 4 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page003.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  36%|███▌      | 5/14 [00:23<00:41,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 5 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page004.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  43%|████▎     | 6/14 [00:28<00:38,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 6 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page005.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  50%|█████     | 7/14 [00:33<00:34,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 7 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page006.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  57%|█████▋    | 8/14 [00:38<00:29,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 8 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page007.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  64%|██████▍   | 9/14 [00:43<00:23,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 9 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page008.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  71%|██████▍  | 10/14 [00:48<00:20,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 10 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page009.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  79%|███████  | 11/14 [00:53<00:15,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 11 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page010.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  86%|███████▋ | 12/14 [00:59<00:10,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 12 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page011.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf':  93%|████████▎| 13/14 [01:05<00:05,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 13 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page012.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text from 'ARAGOG_Advanced_RAG_Output_Grading.pdf': 100%|█████████| 14/14 [01:09<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ARAGOG_Advanced_RAG_Output_Grading.pdf - Page 14 to ARAGOG_Advanced_RAG_Output_Grading/ARAGOG_Advanced_RAG_Output_Grading_page013.txt\n",
      "Text extraction complete for all PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "book = \"ARAGOG_Advanced_RAG_Output_Grading\"\n",
    "os.makedirs(book, exist_ok=True)\n",
    "\n",
    "pdf_path = f\"{book}.pdf\"\n",
    "# Convert each page of the PDF into an image (PNG format by default).\n",
    "# The '500' argument specifies the DPI (dots per inch) for better OCR accuracy.\n",
    "pages = convert_from_path(pdf_path, 500)\n",
    "\n",
    "# Extract the base filename (without the .pdf extension) for naming output text files.\n",
    "# This will be used to create paths like 'book/my_novel_page001.txt'.\n",
    "base_file_name = os.path.basename(pdf_path)[:-4]\n",
    "\n",
    "# Loop through each image (page) generated from the PDF.\n",
    "# imgBlob is the image object, pageNum is its index (starting from 0).\n",
    "for pageNum, imgBlob in tqdm(enumerate(pages), total=len(pages), desc=f\"Extracting text from '{pdf_path}'\"):\n",
    "    # Use pytesseract to perform OCR on the image.\n",
    "    # 'lang='eng'' specifies that the text is in English.\n",
    "    text = pytesseract.image_to_string(imgBlob, lang='eng')\n",
    "\n",
    "    # Format the page number to have leading zeros (e.g., 001, 002, 010).\n",
    "    # This helps in keeping the files sorted correctly.\n",
    "    padded_pageNum = str(pageNum).zfill(3)\n",
    "\n",
    "    # Construct the full path for the output text file.\n",
    "    # It will be saved inside the 'book' directory.\n",
    "    output_file_path = os.path.join(book, f'{base_file_name}_page{padded_pageNum}.txt')\n",
    "\n",
    "    # Open the output file in write mode ('w') and save the extracted text.\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as the_file:\n",
    "        the_file.write(text)\n",
    "\n",
    "    print(f\"Extracted text from {pdf_path} - Page {pageNum+1} to {output_file_path}\")\n",
    "\n",
    "print(\"Text extraction complete for all PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c360f1-bab5-4a43-914c-75c1ff66c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob(f\"{book}/*\"))\n",
    "text = \"\"\n",
    "\n",
    "# Concatenate all files from the book\n",
    "for fl in files:\n",
    "    try:\n",
    "        with open(fl, 'r') as file_:\n",
    "            content = file_.read()\n",
    "            text += content + \"\\n\"\n",
    "    except FileNotFoundError:\n",
    "        print(f\"An error in {nombre_archivo} \")\n",
    "\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "with open(\"{}.txt\".format(book), 'w') as file_:\n",
    "    file_.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a45d0c5-b033-4f46-a727-029899084c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"{}.txt\".format(book))\n",
    "pages_ocr = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4223028c-9015-45f2-a6fe-9acd4983c677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2404.01037v1 [cs.CL] 1 Apr 2024 ar X1V ARAGOG:;: A'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_ocr[0].page_content[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5deb0-2c35-4d41-8d66-eb73d10a74b7",
   "metadata": {},
   "source": [
    "### By using PyDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2a43bb-023d-45f5-a8ab-970633accab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(f\"{book}.pdf\")\n",
    "pages_pydfloader = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b01b092-0cfc-47b0-b149-9c54a566a19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARAGOG: Advanced RAG Output Grading\\nMatouˇ s Eibic'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_pydfloader[0].page_content[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803aa1d-ec06-4004-a407-173b726b2d71",
   "metadata": {},
   "source": [
    "### PDFplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7e81ea2-3121-4d45-96d7-9736ebd1ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_plumber = []\n",
    "\n",
    "with pdfplumber.open(f\"{book}.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        pages_plumber.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd52b78-45de-4c88-902c-766dc4c49da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARAGOG: Advanced RAG Output Grading\\nMatouˇs Eibich'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_plumber[0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997b860-f49b-451a-8ffd-fe182b7ff7d1",
   "metadata": {},
   "source": [
    "# Split text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36be82-73d2-474d-8866-844ba166ee5c",
   "metadata": {},
   "source": [
    "The `RecursiveCharacterTextSplitter` code efficiently breaks down large PDF texts into smaller segments, or chunks with specified `chunk_size` and `chunk_overlap`. This is crucial because Large Language Models (LLMs) have limited input capacities preventing them from processing entire documents at once. By segmenting the text, the code ensures that data fits within an LLM's context window. The overlap between chunks is vital for maintaining semantic continuity preventing the loss of meaning that might occur if a concept spans across a split. This process is especially important for **Retrieval-Augmented Generation (RAG)** systems, as it enables the retrieval of highly relevant, manageable text snippets, leading to more accurate LLM responses and optimizing processing time and costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e31a3d-a8db-43a8-80f6-0993f40da21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(pages_pydfloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6db7d8-6f04-4ae1-a572-4ae561ac9302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2024-04-02T04:16:02+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2024-04-02T04:16:02+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'ARAGOG_Advanced_RAG_Output_Grading.pdf',\n",
       " 'total_pages': 14,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192971f0-fe05-445d-9510-d85725622a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARAGOG: Advanced RAG Output Grading\\nMatouˇ s Eibic'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].page_content[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b34d9e-5d81-4719-a7a1-60632777ede6",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb07a5c-bc57-40ef-807c-eb64e051c307",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c566058-88da-4a60-9ddf-5289e3b50cc6",
   "metadata": {},
   "source": [
    "The code `db = FAISS.from_documents(splits, OpenAIEmbeddings())` converts text chunks into numerical embeddings using **OpenAIEmbeddings**, then stores and indexes these semantic representations in a **FAISS database**. This process is vital as it enables computers to understand and compare text by meaning facilitates efficient similarity searches** across large datasets, and forms the core of Retrieval-Augmented Generation (RAG) systems for feeding LLMs relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ecca8e3-8c39-41cc-bdbc-30b10de2f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(\n",
    "    splits,\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33611065-e7da-4330-9d72-7ba1d5c23eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what does the document is about?\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e7306e-bca4-4d92-b866-51b85a4223f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 Document summary index\n",
      "The Document Summary Index method enhances RAG systems by indexing document summaries for\n",
      "efficient retrieval, while providing LLMs with full text documents for response generation (Liu, 2023a).\n",
      "This decoupling strategy optimizes retrieval speed and accuracy through summary-based indexing and\n",
      "supports comprehensive response synthesis by utilizing the original text.\n",
      "2.3 HyDE\n",
      "The Hypothetical Document Embedding (Gao et al., 2022) technique enhances the document retrieval\n",
      "***********************\n",
      "The Hypothetical Document Embedding (Gao et al., 2022) technique enhances the document retrieval\n",
      "by leveraging LLMs to generate a hypothetical answer to a query. HyDE capitalizes on the ability of\n",
      "LLMs to produce context-rich answers, which, once embedded, serve as a powerful tool to refine and\n",
      "focus document retrieval efforts. See Figure 2 for overview of HyDE RAG system workflow.\n",
      "Figure 2: The process flow of Hypothetical Document Embedding (HyDE) technique within a Retrieval-\n",
      "***********************\n",
      "Figure 2: The process flow of Hypothetical Document Embedding (HyDE) technique within a Retrieval-\n",
      "Augmented Generation system. The diagram illustrates the steps from the initial query input to the generation\n",
      "of a hypothetical answer and its use in retrieving relevant documents to inform the final generated response.\n",
      "2.4 Multi-query\n",
      "The Multi-query technique (Langchain, 2023) enhances document retrieval by expanding a single user\n",
      "***********************\n",
      "another, i.e. Sentence Window, Naive RAG and Document Summary with Cohere rerank. Utilizing\n",
      "plain Document Summary without enhancements was not feasible for this analysis, as it aggregates\n",
      "multiple chunks into one summary, leading to results not directly comparable to other techniques that\n",
      "operate on different chunk quantities.\n",
      "Technique Comparison Mean Diff. P-adj Reject Null\n",
      "Doc Summ Index + Cohere Rerank Classic VDB + Naive RAG 0.0545 0.0000 True\n",
      "***********************\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.page_content)\n",
    "    print(\"***********************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00336a15-d271-48a6-a336-b5766c049f58",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438d151-6582-4fc0-90bb-32144126d276",
   "metadata": {},
   "source": [
    "The following code creates and saves a persistent vector database using ChromaDB. It takes your text splits (document chunks) and converts them into embeddings via OpenAIEmbeddings (using the text-embedding-3-small model). These embeddings, along with the original text, are then stored in a specified local directory ({}_embeddings). Finally, vectorstore.persist() ensures this database is saved to disk, allowing you to reload it later without re-embedding everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56e60cd-23d7-43dc-a918-aa7a3f1a7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136545/904934379.py:6: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    persist_directory=\"{}_embeddings\".format(book))\n",
    "\n",
    "vectorstore.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88aa38e1-6087-4fb2-b6ec-4b8e8f2882be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = (\n",
    "    vectorstore.as_retriever(\n",
    "        search_type=\"similarity\", \n",
    "        search_kwargs={\"k\":5})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "075c579c-bc2f-4e71-bf25-7b2fb2a10d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 Document summary index\n",
      "The Document Summary Index method enhances RAG systems by indexing document summaries for\n",
      "efficient retrieval, while providing LLMs with full text documents for response generation (Liu, 2023a).\n",
      "This decoupling strategy optimizes retrieval speed and accuracy through summary-based indexing and\n",
      "supports comprehensive response synthesis by utilizing the original text.\n",
      "2.3 HyDE\n",
      "The Hypothetical Document Embedding (Gao et al., 2022) technique enhances the document retrieval\n",
      "***********************\n",
      "The Hypothetical Document Embedding (Gao et al., 2022) technique enhances the document retrieval\n",
      "by leveraging LLMs to generate a hypothetical answer to a query. HyDE capitalizes on the ability of\n",
      "LLMs to produce context-rich answers, which, once embedded, serve as a powerful tool to refine and\n",
      "focus document retrieval efforts. See Figure 2 for overview of HyDE RAG system workflow.\n",
      "Figure 2: The process flow of Hypothetical Document Embedding (HyDE) technique within a Retrieval-\n",
      "***********************\n",
      "Figure 2: The process flow of Hypothetical Document Embedding (HyDE) technique within a Retrieval-\n",
      "Augmented Generation system. The diagram illustrates the steps from the initial query input to the generation\n",
      "of a hypothetical answer and its use in retrieving relevant documents to inform the final generated response.\n",
      "2.4 Multi-query\n",
      "The Multi-query technique (Langchain, 2023) enhances document retrieval by expanding a single user\n",
      "***********************\n",
      "another, i.e. Sentence Window, Naive RAG and Document Summary with Cohere rerank. Utilizing\n",
      "plain Document Summary without enhancements was not feasible for this analysis, as it aggregates\n",
      "multiple chunks into one summary, leading to results not directly comparable to other techniques that\n",
      "operate on different chunk quantities.\n",
      "Technique Comparison Mean Diff. P-adj Reject Null\n",
      "Doc Summ Index + Cohere Rerank Classic VDB + Naive RAG 0.0545 0.0000 True\n",
      "***********************\n",
      "RAG, warranting further investigation into its application. Document summary index performance\n",
      "is similar to the best setting of Classic VDB, indicating that with further enhancements, Document\n",
      "summary technique would surpass Classic VDB.\n",
      "Figure 7: Boxplot of Retrieval Precision by Experiment. Each boxplot demonstrates the range and dis-\n",
      "tribution of retrieval precision scores across different RAG techniques. Higher median values and tighter\n",
      "interquartile ranges suggest better performance and consistency.\n",
      "***********************\n"
     ]
    }
   ],
   "source": [
    "for doc in retriever.invoke(\"what does the document is about?\"):\n",
    "    print(doc.page_content)\n",
    "    print(\"***********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4a14ebd-0aff-46fa-acf6-461405715939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what does the document is about?',\n",
       " 'result': 'The document discusses various techniques and strategies to improve retrieval-augmented generation (RAG) systems. It covers methods such as the Document Summary Index, Hypothetical Document Embedding (HyDE), and Multi-query techniques, highlighting their roles in enhancing document retrieval accuracy and response generation efficiency. The document also compares the performance of these methods, indicating that the Document Summary Index and other enhanced techniques have potential for superior retrieval precision compared to traditional approaches.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "query = \"what does the document is about?\"\n",
    "result = chain({\"query\": query})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "criptos",
   "language": "python",
   "name": "criptos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
